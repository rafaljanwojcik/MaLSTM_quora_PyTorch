{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-20 23:50:09,478 | Begining job. All files and logs will be saved at: logs\\train_job_2020-09-20\\siam_lstm\n",
      "2020-09-20 23:50:09,482 | Building Embedding Matrix...\n",
      "2020-09-20 23:50:09,484 | Initializing embeddings vocab...\n",
      "2020-09-20 23:50:22,252 | Embeddings initialized with 400000 words, with 100 embedding dimensions\n",
      "2020-09-20 23:50:22,571 | Reading Dataset and splitting into train and test datasets with seed: 44\n",
      "2020-09-20 23:50:23,439 | Preprocessing Train Dataset...\n",
      "2020-09-20 23:50:23,455 | Cleaning train dataset...\n",
      "2020-09-20 23:50:46,007 | Replacing all words in train dataset with their ids...\n",
      "2020-09-20 23:50:50,773 | Preprocessing Test Dataset...\n",
      "2020-09-20 23:50:50,776 | Cleaning test dataset...\n",
      "2020-09-20 23:50:53,224 | Replacing all words in test dataset with their ids...\n",
      "2020-09-20 23:50:53,638 | \n",
      "2020-09-20 23:50:53,639 | Number of training samples        :364287\n",
      "2020-09-20 23:50:53,640 | Number of validation samples      :40000\n",
      "2020-09-20 23:50:53,641 | Number of unique words          :400001\n",
      "2020-09-20 23:50:53,642 | \n",
      "2020-09-20 23:50:57,830 | Building model.\n",
      "2020-09-20 23:50:57,832 | --------------------------------------\n",
      "2020-09-20 23:50:57,833 | Model Parameters:\n",
      "2020-09-20 23:50:57,834 | Hidden Size                  :50\n",
      "2020-09-20 23:50:57,835 | Number of layers             :2\n",
      "2020-09-20 23:50:57,836 | Use pretrained Embeddings    :True\n",
      "2020-09-20 23:50:57,836 | Dimensions of Embeddings     :100\n",
      "2020-09-20 23:50:57,837 | Train/fine tune Embeddings   :True\n",
      "2020-09-20 23:50:57,838 | Gradient clipping            :1.25\n",
      "2020-09-20 23:50:57,840 | --------------------------------------\n",
      "2020-09-20 23:50:57,840 | Training Parameters:\n",
      "2020-09-20 23:50:57,841 | Device                       : GPU\n",
      "2020-09-20 23:50:57,842 | Optimizer                    : Adam\n",
      "2020-09-20 23:50:57,843 | Loss function                : MSE\n",
      "2020-09-20 23:50:57,844 | Batch Size                   :128\n",
      "2020-09-20 23:50:57,845 | Number of Epochs             :2\n",
      "2020-09-20 23:50:57,846 | --------------------------------------\n",
      "2020-09-20 23:50:57,847 | Training the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccdd417e7bb4e4487cf7ad0a88b8977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2846.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-20 23:50:57,982 | Mean loss till 0th iteration of epoch 0: 0.6095460653305054\n",
      "2020-09-20 23:52:07,110 | Mean loss till 1000th iteration of epoch 0: 0.25299269778209255\n",
      "2020-09-20 23:53:16,749 | Mean loss till 2000th iteration of epoch 0: 0.23549820729966522\n",
      "\n",
      "2020-09-20 23:54:25,526 | Saving best model at: logs\\train_job_2020-09-20\\siam_lstm\\best_model\\checkpoint.pth\n",
      "2020-09-20 23:54:26,695 | Mean loss and accuracy of epoch 0 - train: 0.22219446230776613, 0.6463, test: 0.36922279223799703, 0.5315. Calculation time: 0.05801304307248857 hours\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7566dcb96942a99329655a8b238944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2846.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-20 23:54:26,800 | Mean loss till 0th iteration of epoch 1: 0.16058523952960968\n",
      "2020-09-20 23:55:35,997 | Mean loss till 1000th iteration of epoch 1: 0.17745866395019508\n",
      "2020-09-20 23:56:45,165 | Mean loss till 2000th iteration of epoch 1: 0.17403074264571047\n",
      "\n",
      "2020-09-20 23:57:53,902 | Saving best model at: logs\\train_job_2020-09-20\\siam_lstm\\best_model\\checkpoint.pth\n",
      "2020-09-20 23:57:55,370 | Mean loss and accuracy of epoch 1 - train: 0.17185391268666078, 0.7566, test: 0.27834498360753057, 0.6323. Calculation time: 0.05796466198232439 hours\n",
      "2020-09-20 23:57:55,371 | Model training finished in: 6.959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "from datetime import date\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.data.data import ImportData, QuoraQuestionDataset\n",
    "from modules.models.embeddings import EmbeddedVocab\n",
    "from modules.models.models import SiameseLSTM\n",
    "from modules.utils.utils import collate_fn_lstm, train, eval, setup_logger\n",
    "\n",
    "\n",
    "today = str(date.today())\n",
    "path = Path(f'./logs/train_job_{today}/')\n",
    "emb_path = Path('./logs/embeddings')\n",
    "data_path = Path('./logs/data')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-model\", \"--model_name\", type=str, help=\"Name of trained model. Needed only for correct logs output\", default='siam_lstm')  \n",
    "parser.add_argument(\"-log\", \"--logdir\", type=str, help=\"Directory to save all downloaded files, and model checkpoints.\", default=path)  \n",
    "parser.add_argument(\"-df\", \"--data_file\", type=str, help=\"Path to dataset.\", default=data_path/\"dataset.csv\")\n",
    "parser.add_argument(\"-pr\", \"--use_pretrained\", action='store_true', help=\"Boolean, whether use pretrained embeddings.\", default=True)\n",
    "parser.add_argument(\"-nodwl\", \"--download_emb\",  action='store_true', help=\"Bool, whether to download embeddings or not (default is to download 100 dimensional Glove embeddings)\", default=False)\n",
    "parser.add_argument(\"-dim\", \"--emb_dim\", type=int, help=\"Dimensions of pretrained embeddings\", default=100)\n",
    "parser.add_argument(\"-empth\", \"--emb_path\", type=str, help=\"path to file with pretrained embeddings\", default=emb_path)\n",
    "parser.add_argument(\"-s\", \"--split_seed\", type=int, help=\"Seed for splitting the dataset.\", default=44)\n",
    "parser.add_argument('-noprep', \"--preprocessing\", action='store_false', help=\"Preprocess dataset before training the model\", default=True)\n",
    "parser.add_argument(\"-hid\", \"--n_hidden\", type=int, help=\"Number of hidden units in LSTM layer.\", default=50)\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, help=\"Batch Size.\", default=128)\n",
    "parser.add_argument(\"-epo\", \"--n_epoch\", type=int, help=\"Number of epochs.\", default=2)\n",
    "parser.add_argument(\"-nl\", \"--n_layer\", type=int, help=\"Number of LSTM layers.\", default=2)\n",
    "parser.add_argument(\"-gc\", \"--gradient_clipping_norm\", type=float, help=\"Gradient clipping norm\", default=1.25)\n",
    "parser.add_argument(\"-note\", \"--train_embeddings\", action='store_false', help=\"Whether to fine-tune embedding weights during training\", default=True)\n",
    "\n",
    "args = parser.parse_args('')\n",
    "args.logdir = args.logdir/args.model_name\n",
    "model_path = args.logdir/'best_model/'\n",
    "if not args.logdir.exists():\n",
    "    os.makedirs(args.logdir)\n",
    "    \n",
    "logger = setup_logger(str(args.logdir/'logs.log'))\n",
    "logger.info(\"Begining job. All files and logs will be saved at: {}\".format(args.logdir))\n",
    "\n",
    "if args.use_pretrained:\n",
    "    logger.info('Building Embedding Matrix...')\n",
    "    embedded_vocab_class = EmbeddedVocab(args.emb_path/'glove.6B.100d.txt', args.emb_dim, args.download_emb, args.emb_path, logger)\n",
    "else:\n",
    "    embedded_vocab_class = None\n",
    "    \n",
    "logger.info('Reading Dataset and splitting into train and test datasets with seed: {}'.format(args.split_seed))\n",
    "data = ImportData(str(args.data_file))\n",
    "data.train_test_split(seed=args.split_seed)\n",
    "\n",
    "logger.info('Preprocessing Train Dataset...')\n",
    "train_dataset = QuoraQuestionDataset(data.train, use_pretrained_emb=args.use_pretrained, reverse_vocab=embedded_vocab_class.reverse_vocab, preprocess = args.preprocessing, train=True, logger=logger)\n",
    "train_dataset.words_to_ids()\n",
    "logger.info('Preprocessing Test Dataset...')\n",
    "test_dataset = QuoraQuestionDataset(data.test, use_pretrained_emb=True, reverse_vocab=train_dataset.reverse_vocab, preprocess = args.preprocessing, train = False, logger=logger)\n",
    "test_dataset.words_to_ids()\n",
    "\n",
    "\n",
    "logger.info('')\n",
    "logger.info('Number of training samples        :{}'.format(len(train_dataset)))\n",
    "logger.info('Number of validation samples      :{}'.format(len(test_dataset)))\n",
    "logger.info('Number of unique words          :{}'.format(train_dataset.unique_words))\n",
    "logger.info('')\n",
    "\n",
    "n_hidden = args.n_hidden\n",
    "gradient_clipping_norm = args.gradient_clipping_norm\n",
    "batch_size = args.batch_size\n",
    "embeddings_dim = args.emb_dim\n",
    "n_epoch = args.n_epoch\n",
    "n_layer = args.n_layer\n",
    "n_token = train_dataset.unique_words\n",
    "use_pretrained_embeddings = args.use_pretrained\n",
    "train_emb = args.train_embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = collate_fn_lstm)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=False, collate_fn = collate_fn_lstm)\n",
    "\n",
    "model = SiameseLSTM(n_hidden, embedded_vocab_class, embeddings_dim, n_layer, n_token, train_embeddings = train_emb, use_pretrained = use_pretrained_embeddings)\n",
    "model = model.float()\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "logger.info('Building model.')\n",
    "logger.info('--------------------------------------')\n",
    "logger.info('Model Parameters:')\n",
    "logger.info('Hidden Size                  :{}'.format(args.n_hidden))\n",
    "logger.info('Number of layers             :{}'.format(args.n_layer))\n",
    "logger.info('Use pretrained Embeddings    :{}'.format(args.use_pretrained))\n",
    "logger.info('Dimensions of Embeddings     :{}'.format(args.emb_dim))\n",
    "logger.info('Train/fine tune Embeddings   :{}'.format(args.train_embeddings))\n",
    "logger.info('Gradient clipping            :{}'.format(args.gradient_clipping_norm))\n",
    "logger.info('--------------------------------------')\n",
    "logger.info('Training Parameters:')\n",
    "logger.info('Device                       :{}'.format(' GPU' if torch.cuda.is_available() else ' CPU'))\n",
    "logger.info('Optimizer                    :{}'.format(' Adam'))\n",
    "logger.info('Loss function                :{}'.format(' MSE'))\n",
    "logger.info('Batch Size                   :{}'.format(args.batch_size))\n",
    "logger.info('Number of Epochs             :{}'.format(args.n_epoch))\n",
    "logger.info('--------------------------------------')\n",
    "\n",
    "start = time()\n",
    "all_train_losses = []\n",
    "all_test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "best_acc = 0.5\n",
    "logger.info(\"Training the model...\")\n",
    "for epoch in range(n_epoch):\n",
    "    epoch_time = time()\n",
    "    epoch_iteration = 0\n",
    "    epoch_loss=[]\n",
    "    preds_train = []\n",
    "\n",
    "    train(model, optimizer, criterion, train_dataloader, device, epoch_loss, preds_train, args.gradient_clipping_norm, epoch, logger)\n",
    "\n",
    "    eval_loss = []\n",
    "    preds_test = []\n",
    "    eval(model, criterion, test_dataloader, device, eval_loss, preds_test)\n",
    "\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.sum(preds_train)/data.train.shape[0]\n",
    "    test_loss = np.mean(eval_loss)\n",
    "    test_accuracy = np.sum(preds_test)/data.test.shape[0]\n",
    "    \n",
    "    if test_accuracy>best_acc:\n",
    "        if not model_path.exists():\n",
    "            os.mkdir(model_path)\n",
    "        logger.info('Saving best model at: {}'.format(str(model_path/'checkpoint.pth')))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy':test_accuracy\n",
    "            }, str(model_path/'checkpoint.pth'))\n",
    "\n",
    "    all_train_losses.append(train_loss)\n",
    "    all_test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    logger.info('Mean loss and accuracy of epoch {} - train: {}, {}, test: {}, {}. Calculation time: {} hours'.format(epoch, train_loss, round(train_accuracy, 4), test_loss, round(test_accuracy, 4), (time() - epoch_time)/3600))\n",
    "\n",
    "logger.info(\"Model training finished in: {}\".format(np.round((time()-start)/60, 3)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(f'Train and test losses during training of {args.model_name} model')\n",
    "plt.plot(list(range(len(all_train_losses))), all_train_losses, label='train')\n",
    "plt.plot(list(range(len(all_test_losses))), all_test_losses, label='test')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(args.logdir/'loss_plots.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(f'Train and test losses during training of {args.model_name} model')\n",
    "plt.plot(list(range(len(train_accuracies))), train_accuracies, label='train')\n",
    "plt.plot(list(range(len(test_accuracies))), test_accuracies, label='test')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig(args.logdir/'acc_plots.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
