{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/training.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-21 23:04:20,620 | Begining job. All files and logs will be saved at: logs\\train_job_2020-09-21\\bert\n",
      "2020-09-21 23:04:20,621 | Reading Dataset and splitting into train and test datasets with seed: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafal.wojcik\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-21 23:04:21,515 | \n",
      "2020-09-21 23:04:21,516 | Number of training samples        :364287\n",
      "2020-09-21 23:04:21,517 | Number of validation samples      :40000\n",
      "2020-09-21 23:04:21,518 | \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "from datetime import date\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
    "\n",
    "from modules.data import ImportData\n",
    "from modules.models import SiameseBERT\n",
    "from modules.utils import collate_fn_bert, setup_logger, compute_metrics, get_quora_huggingface\n",
    "\n",
    "\n",
    "today = str(date.today())\n",
    "path = Path(f'./logs/train_job_{today}/')\n",
    "emb_path = Path('./logs/embeddings')\n",
    "data_path = Path('./logs/data')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-model_name\", \"--model_name\", type=str, help=\"Name of trained model. Needed only for correct logs output\", default='bert')  \n",
    "parser.add_argument(\"-log\", \"--logdir\", type=str, help=\"Directory to save all downloaded files, and model checkpoints.\", default=path)  \n",
    "parser.add_argument(\"-df\", \"--data_file\", type=str, help=\"Path to dataset.\", default=data_path/\"dataset.csv\")\n",
    "parser.add_argument(\"-s\", \"--split_seed\", type=int, help=\"Seed for splitting the dataset.\", default=44)\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, help=\"Batch Size.\", default=8)\n",
    "parser.add_argument(\"-epo\", \"--n_epoch\", type=int, help=\"Number of epochs.\", default=4)\n",
    "parser.add_argument(\"-bert_cls\", \"--bert_cls\", type=str, help=\"Type of BERT trained (classificator, siamese).\", default='classifier')\n",
    "parser.add_argument(\"-bert_backbone\", \"--bert_backbone\", type=str, help=\"Either path to the model, or name of the BERT model that should be used, compatible with HuggingFace Transformers.\", default='bert-base-uncased')\n",
    "\n",
    "args = parser.parse_args('')\n",
    "args.logdir = args.logdir/args.model_name\n",
    "model_path = args.logdir/'best_model/'\n",
    "if not args.logdir.exists():\n",
    "    os.makedirs(args.logdir)\n",
    "\n",
    "logger = setup_logger(str(args.logdir/'logs.log'))\n",
    "logger.info(\"Begining job. All files and logs will be saved at: {}\".format(args.logdir))\n",
    "\n",
    "\n",
    "logger.info('Reading Dataset and splitting into train and test datasets with seed: {}'.format(args.split_seed))\n",
    "data = ImportData(str(args.data_file))\n",
    "data.train_test_split(seed=args.split_seed)\n",
    "\n",
    "\n",
    "logger.info('')\n",
    "logger.info('Number of training samples        :{}'.format(len(data.train)))\n",
    "logger.info('Number of validation samples      :{}'.format(len(data.test)))\n",
    "logger.info('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(args.logdir/'results'),          # output directory\n",
    "    overwrite_output_dir = True,\n",
    "    evaluate_during_training = True,\n",
    "    logging_first_step = True,\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01, # strength of weight decay\n",
    "    logging_dir=str(args.logdir/'logs'),            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    data_collator=lambda x: collate_fn_classifier_bert(x, tokenizer, args.bert_cls), \n",
    "    train_dataset=data.train.values,         # training dataset\n",
    "    eval_dataset=data.test.values,\n",
    "    compute_metrics = compute_metrics# evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "from datetime import date\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.data import ImportData, QuoraQuestionDataset\n",
    "from modules.embeddings import EmbeddedVocab\n",
    "from modules.models import SiameseBERT, ClassifierBERT\n",
    "from modules.utils import collate_fn_bert, train_bert, eval_bert, setup_logger\n",
    "\n",
    "\n",
    "today = str(date.today())\n",
    "path = Path(f'./logs/train_job_{today}/')\n",
    "emb_path = Path('./logs/embeddings')\n",
    "data_path = Path('./logs/data')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-model_name\", \"--model_name\", type=str, help=\"Name of trained model. Needed only for correct logs output\", default='bert')  \n",
    "parser.add_argument(\"-log\", \"--logdir\", type=str, help=\"Directory to save all downloaded files, and model checkpoints.\", default=path)  \n",
    "parser.add_argument(\"-df\", \"--data_file\", type=str, help=\"Path to dataset.\", default=data_path/\"dataset.csv\")\n",
    "parser.add_argument(\"-s\", \"--split_seed\", type=int, help=\"Seed for splitting the dataset.\", default=44)\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, help=\"Batch Size.\", default=8)\n",
    "parser.add_argument(\"-epo\", \"--n_epoch\", type=int, help=\"Number of epochs.\", default=4)\n",
    "parser.add_argument(\"-bert_cls\", \"--bert_cls\", type=str, help=\"Type of BERT trained (classificator, siamese).\", default='classifier')\n",
    "parser.add_argument(\"-bert_backbone\", \"--bert_backbone\", type=str, help=\"Either path to the model, or name of the BERT model that should be used, compatible with HuggingFace Transformers.\", default='bert-base-uncased')\n",
    "\n",
    "args = parser.parse_args('')\n",
    "args.logdir = args.logdir/args.model_name\n",
    "model_path = args.logdir/'best_model/'\n",
    "if not args.logdir.exists():\n",
    "    os.makedirs(args.logdir)\n",
    "\n",
    "logger = setup_logger(str(args.logdir/'logs.log'))\n",
    "logger.info(\"Begining job. All files and logs will be saved at: {}\".format(args.logdir))\n",
    "\n",
    "\n",
    "logger.info('Reading Dataset and splitting into train and test datasets with seed: {}'.format(args.split_seed))\n",
    "data = ImportData(str(args.data_file))\n",
    "data.train_test_split(seed=args.split_seed)\n",
    "\n",
    "\n",
    "logger.info('')\n",
    "logger.info('Number of training samples        :{}'.format(len(data.train)))\n",
    "logger.info('Number of validation samples      :{}'.format(len(data.test)))\n",
    "logger.info('')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataloader = DataLoader(data.train.values, batch_size = args.batch_size, shuffle=True, collate_fn=collate_fn_bert)\n",
    "test_dataloader = DataLoader(data.test.values, batch_size= args.batch_size, shuffle=False, collate_fn=collate_fn_bert)\n",
    "\n",
    "model = SiameseBERT(args.bert_backbone, device) if args.bert_cls=='siamese' else ClassifierBERT(args.bert_backbone, device)\n",
    "model = model.float()\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss() if args.bert_cls=='siamese' else nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "logger.info('Building model.')\n",
    "logger.info('--------------------------------------')\n",
    "logger.info('Model Parameters:')\n",
    "logger.info('Bert Backbone:               :{}'.format(args.bert_backbone))\n",
    "logger.info('--------------------------------------')\n",
    "logger.info('Training Parameters:')\n",
    "logger.info('Device                       :{}'.format(str(device)))\n",
    "logger.info('Optimizer                    :{}'.format(' Adam'))\n",
    "logger.info('Loss function                :{}'.format('MSE' if args.bert_cls == 'siamese' else 'CE'))\n",
    "logger.info('Batch Size                   :{}'.format(args.batch_size))\n",
    "logger.info('Number of Epochs             :{}'.format(args.n_epoch))\n",
    "logger.info('--------------------------------------')\n",
    "\n",
    "start = time()\n",
    "all_train_losses = []\n",
    "all_test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "best_acc = 0.5\n",
    "logger.info(\"Training the model...\")\n",
    "for epoch in range(args.n_epoch):\n",
    "    epoch_time = time()\n",
    "    epoch_iteration = 0\n",
    "    epoch_loss=[]\n",
    "    preds_train = []\n",
    "\n",
    "    train_bert(model, optimizer, criterion, train_dataloader, device, epoch_loss, preds_train, epoch, logger)\n",
    "\n",
    "    eval_loss = []\n",
    "    preds_test = []\n",
    "    eval_bert(model, criterion, test_dataloader, device, eval_loss, preds_test)\n",
    "\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.sum(preds_train)/data.train.shape[0]\n",
    "    test_loss = np.mean(eval_loss)\n",
    "    test_accuracy = np.sum(preds_test)/data.test.shape[0]\n",
    "\n",
    "    if test_accuracy>best_acc:\n",
    "        if not model_path.exists():\n",
    "            os.mkdir(model_path)\n",
    "        logger.info('Saving best model at: {}'.format(str(model_path/'checkpoint.pth')))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy':test_accuracy\n",
    "            }, str(model_path/'checkpoint.pth'))\n",
    "\n",
    "    all_train_losses.append(train_loss)\n",
    "    all_test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    logger.info('Mean loss and accuracy of epoch {} - train: {}, {}, test: {}, {}. Calculation time: {} hours'.format(epoch, train_loss, round(train_accuracy, 4), test_loss, round(test_accuracy, 4), (time() - epoch_time)/3600))\n",
    "\n",
    "logger.info(\"Model training finished in: {}\".format(np.round((time()-start)/60, 3)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(f'Train and test losses during training of {args.model_name} model')\n",
    "plt.plot(list(range(len(all_train_losses))), all_train_losses, label='train')\n",
    "plt.plot(list(range(len(all_test_losses))), all_test_losses, label='test')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(args.logdir/'loss_plots.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(f'Train and test losses during training of {args.model_name} model')\n",
    "plt.plot(list(range(len(train_accuracies))), train_accuracies, label='train')\n",
    "plt.plot(list(range(len(test_accuracies))), test_accuracies, label='test')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig(args.logdir/'acc_plots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do wczytywania pretrenowanego BERTa z poporzednich treningow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save(self, output_path: str):\n",
    "        self.bert.save_pretrained(output_path)\n",
    "        self.tokenizer.save_pretrained(output_path)\n",
    "\n",
    "        with open(os.path.join(output_path, 'sentence_bert_config.json'), 'w') as fOut:\n",
    "            json.dump(self.get_config_dict(), fOut, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path: str):\n",
    "        with open(os.path.join(input_path, 'sentence_bert_config.json')) as fIn:\n",
    "            config = json.load(fIn)\n",
    "        return BERT(model_name_or_path=input_path, **config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
